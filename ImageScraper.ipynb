{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageScraper",
      "provenance": [],
      "collapsed_sections": [
        "ONIqIEu7VyT4",
        "pvM0imifGwOY"
      ],
      "mount_file_id": "17f9l2cUPK2Yis_4A_YzzeEOqOPafTy5t",
      "authorship_tag": "ABX9TyPt0iprwZPG69pShYkMDa+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joedockrill/image-scraper/blob/master/ImageScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONIqIEu7VyT4",
        "colab_type": "text"
      },
      "source": [
        "# DuckDuckGo and Google Image Scraper\n",
        "\n",
        "This notebook is an image scraper for creating deep learning datasets. Expand this section for help. \n",
        "\n",
        "Hugs & kisses, Joe Dockrill. \n",
        "\n",
        "credits: \n",
        "- [Deepan Prabhu Babu](https://github.com/deepanprabhu/duckduckgo-images-api) for the base DuckDuckGo code\n",
        "- Iegor Timukhin for pointing out that the search constraints param was sitting under my nose the whole time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zga2kYYG1b4R",
        "colab_type": "text"
      },
      "source": [
        "This notebook can scrape from Google and DuckDuckGo but Google is really just an emergency backup in case the DuckDuckGo code breaks at some point.\n",
        "\n",
        "The thumbnails from DDG are larger, the search options are better and the results include the original (full sized) image url which you can have downloaded instead of a thumbnail by using an img_size.\n",
        "\n",
        "Bear in mind that you will get more failures downloading original images because of out of date links, truncated downloads, and sites which ban hot-linking.\n",
        "\n",
        "**Version 2** \\\n",
        "\\\n",
        "You can now constrain DDG searches as follows:\n",
        "\n",
        "```\n",
        "duckduckgo_search(label: str, keywords: str, max_results: int=100,\n",
        "                      img_size: ImgSize=ImgSize.Thumbs, \n",
        "                      img_type: ImgType=ImgType.Photo,\n",
        "                      img_layout: ImgLayout=ImgLayout.Square,\n",
        "                      img_color: ImgColor=ImgColor.All) -> None:\n",
        "\n",
        "img_size can be one of the following: (default=ImgSize.Thumbs)\n",
        "Thumbs, Small, Medium, Large, Wallpaper\n",
        " \n",
        "img_type can be one of the following: (default=ImgType.Photo)\n",
        "All, Photo, Clipart, Gif, Transparent\n",
        "\n",
        "img_layout can be one of the following: (default=ImgLayout.Square)\n",
        "All, Square, Tall, Wide\n",
        "  \n",
        "img_color can be one of the following: (default = ImgColor.All)\n",
        "All, Color, Monochrome, Red, Orange, Yellow, Green, Blue, Purple, Pink, Brown, Black, Gray, Teal, White\n",
        "```\n",
        "\n",
        "Workflow:\n",
        "- Write some search functions in the \"Download your images here\" cell\n",
        "- Run the image cleaner to delete rubbish\n",
        "- Zip it all up\n",
        "- Download it or copy it to Google Drive\n",
        "\n",
        "Images will be downloaded into folders by label name. If you want a one level zip file with all the images at the root just pass an empty string as a label name.\n",
        "\n",
        "If you would prefer to create a CSV file of label/url pairs you can do that at the bottom of the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvBgNgI7W1lT",
        "colab_type": "text"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVwjN8fnD5kc",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title RUN THIS CELL.\n",
        "#@markdown If you want to see the code, select this cell, click the ... menu in the top right of\n",
        "#@markdown the cell then click Form->Hide Form\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interactive\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from typing import Callable\n",
        "from enum import Enum\n",
        "\n",
        "BASE_FOLDER = \"images\"\n",
        "\n",
        "##########################################################################################\n",
        "# scraping\n",
        "##########################################################################################\n",
        "def google_scrape_urls(keywords: str, max_results: int) -> list:\n",
        "  '''scrape urls from google image search'''\n",
        "  BASE_URL = \"https://www.google.com/search?site=&tbm=isch&source=hp&biw=1873&bih=990&q=\"\n",
        "\n",
        "  HEADERS = {\n",
        "      'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
        "      'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "      'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
        "      'Accept-Encoding': 'none',\n",
        "      'Accept-Language': 'en-US,en;q=0.8',\n",
        "      'Connection': 'keep-alive',\n",
        "  }\n",
        "  \n",
        "  searchurl = BASE_URL + keywords\n",
        "  resp = requests.get(searchurl, headers=HEADERS)\n",
        "  html = resp.text\n",
        "  \n",
        "  soup = BeautifulSoup(html, \"html.parser\")\n",
        "  results = soup.findAll(\"img\", {\"data-src\":True}, limit=max_results)\n",
        "  \n",
        "  links = []\n",
        "  for re in results:\n",
        "    links.append(re[\"data-src\"])\n",
        "\n",
        "  return links  \n",
        "\n",
        "class ImgSize(Enum):\n",
        "  Thumbs=\"\"\n",
        "  Small=\"Small\"\n",
        "  Medium=\"Medium\"\n",
        "  Large=\"Large\"\n",
        "  Wallpaper=\"Wallpaper\"\n",
        "\n",
        "class ImgType(Enum):\n",
        "  All=\"\"\n",
        "  Photo=\"photo\"\n",
        "  Clipart=\"clipart\"\n",
        "  Gif=\"gif\"\n",
        "  Transparent=\"transparent\"\n",
        "\n",
        "class ImgLayout(Enum):\n",
        "  All=\"\"\n",
        "  Square=\"Square\"\n",
        "  Tall=\"Tall\"\n",
        "  Wide=\"Wide\"\n",
        "  \n",
        "class ImgColor(Enum):\n",
        "  All=\"\"\n",
        "  Color=\"color\"\n",
        "  Monochrome=\"Monochrome\"\n",
        "  Red=\"Red\"\n",
        "  Orange=\"Orange\"\n",
        "  Yellow=\"Yellow\"\n",
        "  Green=\"Green\"\n",
        "  Blue=\"Blue\"\n",
        "  Purple=\"Purple\"\n",
        "  Pink=\"Pink\" \n",
        "  Brown=\"Brown\"\n",
        "  Black=\"Black\" \n",
        "  Gray=\"Gray\" \n",
        "  Teal=\"Teal\"\n",
        "  White=\"White\"\n",
        "\n",
        "def duckduckgo_scrape_urls(keywords: str, max_results: int, \n",
        "                           img_size: ImgSize=ImgSize.Thumbs, \n",
        "                           img_type: ImgType=ImgType.Photo,\n",
        "                           img_layout: ImgLayout=ImgLayout.Square,\n",
        "                           img_color: ImgColor=ImgColor.All) -> list:\n",
        "  '''scrape urls from duckduckgo image search'''\n",
        "  BASE_URL = 'https://duckduckgo.com/'\n",
        "  params = {\n",
        "    'q': keywords\n",
        "  };\n",
        "  results = 0\n",
        "  links = []\n",
        "\n",
        "  resp = requests.post(BASE_URL, data=params)\n",
        "  match = re.search(r'vqd=([\\d-]+)\\&', resp.text, re.M|re.I)\n",
        "  assert match is not None, \"Failed to obtain search token\"\n",
        "\n",
        "  HEADERS = {\n",
        "      'authority': 'duckduckgo.com',\n",
        "      'accept': 'application/json, text/javascript, */*; q=0.01',\n",
        "      'sec-fetch-dest': 'empty',\n",
        "      'x-requested-with': 'XMLHttpRequest',\n",
        "      'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36',\n",
        "      'sec-fetch-site': 'same-origin',\n",
        "      'sec-fetch-mode': 'cors',\n",
        "      'referer': 'https://duckduckgo.com/',\n",
        "      'accept-language': 'en-US,en;q=0.9',\n",
        "  }\n",
        "\n",
        "  constraints = \"\"\n",
        "  if(img_size != ImgSize.Thumbs): constraints +=  \"size:\" + img_size.name\n",
        "  constraints += \",\"\n",
        "  if(img_type != ImgType.All): constraints +=  \"type:\" + img_type.name\n",
        "  constraints += \",\"\n",
        "  if(img_layout != ImgLayout.All): constraints +=  \"layout:\" + img_layout.name\n",
        "  constraints += \",\"\n",
        "  if(img_color != ImgColor.All): constraints +=  \"color:\" + img_color.name\n",
        "  \n",
        "  PARAMS = (\n",
        "      ('l', 'us-en'),\n",
        "      ('o', 'json'),\n",
        "      ('q', keywords),\n",
        "      ('vqd', match.group(1)),\n",
        "      ('f', constraints),\n",
        "      ('p', '1'),\n",
        "      ('v7exp', 'a'),\n",
        "  )\n",
        "\n",
        "  requestUrl = BASE_URL + \"i.js\"\n",
        "\n",
        "  while True:\n",
        "      while True:\n",
        "          try:\n",
        "              resp = requests.get(requestUrl, headers=HEADERS, params=PARAMS)\n",
        "              data = json.loads(resp.text)\n",
        "              break\n",
        "          except ValueError as e:\n",
        "              print(\"Hit request throttle, sleeping and retrying\")\n",
        "              time.sleep(5); #seems a lot but ok...\n",
        "              continue\n",
        "\n",
        "      #result[\"thumbnail\"] is normally big enough for most purposes\n",
        "      #result[\"width\"], result[\"height\"] are for the full size img in result[\"image\"]\n",
        "      #result[\"image\"] url to full size img on orig site (so may be less reliable) \n",
        "      #result[\"url\"], result[\"title\"].encode('utf-8') from the page the img came from\n",
        "      \n",
        "      for result in data[\"results\"]:\n",
        "        if(img_size == ImgSize.Thumbs): links.append(result[\"thumbnail\"])\n",
        "        else:                       links.append(result[\"image\"])\n",
        "\n",
        "        if(max_results is not None):\n",
        "          if(len(links) >= max_results) : return links\n",
        "\n",
        "      if \"next\" not in data:\n",
        "          #no next page, all done\n",
        "          return links\n",
        "\n",
        "      requestUrl = BASE_URL + data[\"next\"]\n",
        "\n",
        "##########################################################################################\n",
        "# searching & downloading\n",
        "##########################################################################################\n",
        "def google_search(label: str, keywords: str, max_results: int=100) -> None:\n",
        "  '''run a google search and download the images'''\n",
        "  print(\"Google search: \", keywords)\n",
        "  links = google_scrape_urls(keywords,max_results)\n",
        "  download_urls(label, links)\n",
        "\n",
        "def duckduckgo_search(label: str, keywords: str, max_results: int=100,\n",
        "                           img_size: ImgSize=ImgSize.Thumbs, \n",
        "                           img_type: ImgType=ImgType.Photo,\n",
        "                           img_layout: ImgLayout=ImgLayout.Square,\n",
        "                           img_color: ImgColor=ImgColor.All) -> None:\n",
        "  '''run a duckduckgo search and download the images'''\n",
        "  print(\"Duckduckgo search:\", keywords)\n",
        "  links = duckduckgo_scrape_urls(keywords, max_results, img_size, img_type, img_layout, img_color)\n",
        "  download_urls(label, links)\n",
        "\n",
        "def download_urls(label: str, links: list) -> None:\n",
        "  '''downloads urls into the folder for that label'''\n",
        "  if(len(links) == 0):\n",
        "    print(\"Nothing to download!\"); return\n",
        "\n",
        "  print(\"Downloading\", len(links), \"images into\", label)\n",
        "\n",
        "  folder = os.path.join(BASE_FOLDER, label)\n",
        "  if not os.path.exists(folder): os.makedirs(folder)\n",
        "\n",
        "  bar = widgets.IntProgress(0, 0, len(links) - 1)\n",
        "  display(bar)\n",
        "\n",
        "  i = 1\n",
        "  mk_fn = lambda i: os.path.join(folder, label + str(i).zfill(3) + \".jpg\")\n",
        "  is_file = lambda i: os.path.isfile(mk_fn(i))\n",
        "  while is_file(i): i += 1 # don't overwrite previous searches\n",
        "  \n",
        "  for link in links:\n",
        "      try:\n",
        "        resp = requests.get(link)      \n",
        "        fn = mk_fn(i)\n",
        "        with open(fn, \"wb\") as file:\n",
        "            file.write(resp.content)\n",
        "\n",
        "        try:\n",
        "          img = Image.open(fn)\n",
        "          img.verify()\n",
        "          img.close()\n",
        "        except:\n",
        "          print(fn, \"is invalid\")\n",
        "          shutil.os.remove(fn)\n",
        "      except:\n",
        "        print(\"Exception occured while retrieving\", link)\n",
        "\n",
        "      i += 1\n",
        "      bar.value += 1\n",
        "\n",
        "  bar.bar_style = \"success\"\n",
        "\n",
        "def save_urls(filename: str, scrape_func: Callable, label: str, keywords: str, max_results: int) -> None:\n",
        "  '''run a search and concat the urls to a csv'''\n",
        "  if(os.path.isfile(filename) == False):\n",
        "    df = pd.DataFrame(columns=[\"URL\", \"Label\"])\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "  urls = scrape_func(keywords, max_results)\n",
        "  rows = []\n",
        "\n",
        "  for url in urls:\n",
        "    rows.append({\"URL\":url, \"Label\":label})\n",
        "    \n",
        "  df = pd.concat([pd.read_csv(filename), pd.DataFrame(rows)]) \n",
        "  df.to_csv(filename, index=False)\n",
        "\n",
        "##########################################################################################\n",
        "# moving files around\n",
        "##########################################################################################\n",
        "def download_file(filename: str) -> None:\n",
        "  '''trigger a file download from colab to local system'''\n",
        "  files.download(filename)\n",
        "\n",
        "def transfer_to_drive(filename: str, dest_folder: str=\"Datasets\") -> None:\n",
        "  '''transfer file from colab runtime to google drive'''\n",
        "  drive.mount('/content/drive')\n",
        "  folder = os.path.join(\"/content/drive/My Drive\", dest_folder)\n",
        "  if not os.path.exists(folder): os.makedirs(folder)\n",
        "  shutil.copyfile(filename, os.path.join(folder, filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTCCj9TS0WVY",
        "colab_type": "text"
      },
      "source": [
        "**Run this cell to delete all image files (to create another dataset or reset)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYwHbopFG63A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r -f images/*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4dwxx_b0pRV",
        "colab_type": "text"
      },
      "source": [
        "**Download your images here**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vw7K4ULGz-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ZIP_NAME = \"images.zip\" # change this to something more meaningful\n",
        "\n",
        "# and run some searches: (help and options are in the section at the top)\n",
        "#\n",
        "# duckduckgo_search(label: str, keywords: str, max_results: int=100,\n",
        "#                       img_size: ImgSize=ImgSize.Thumbs, \n",
        "#                       img_type: ImgType=ImgType.Photo,\n",
        "#                       img_layout: ImgLayout=ImgLayout.Square,\n",
        "#                       img_color: ImgColor=ImgColor.All) -> None:\n",
        "\n",
        "# EG:\n",
        "# ZIP_NAME = \"Clowns.zip\"\n",
        "# duckduckgo_search(\"Nice\", \"nice clowns\", max_results=20)\n",
        "# duckduckgo_search(\"Scary\", \"scary clowns\", max_results=20)\n",
        "\n",
        "\n",
        "# you can also use google_search() if you prefer or if the ddg code breaks.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-dTEWUKn3C",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Quick & Dirty Dataset Cleaner \n",
        "#@markdown Run this cell for a quick image cleaner. When you hit delete it's done immediately but \n",
        "#@markdown you'll need to run the cell again or swap folders to refresh the view.\n",
        "\n",
        "#@markdown This is SLOW at loading a lot of images. Pagination is on my list of things to do.\n",
        "\n",
        "\n",
        "def click_handler(btn):\n",
        "  shutil.os.remove(btn.tag)\n",
        "  btn.disabled = True\n",
        "\n",
        "def render_image_cleaner(folder):\n",
        "  items = []\n",
        "  if(folder == \"/\"): folder = \"\"\n",
        "  path = os.path.join(BASE_FOLDER, folder)\n",
        "  \n",
        "  for filename in os.listdir(path):\n",
        "      if filename.endswith(\".jpg\"):\n",
        "          file = open(os.path.join(path, filename), \"rb\")\n",
        "          fstream = file.read()\n",
        "          img = widgets.Image(value=fstream, format='jpg')\n",
        "          img.layout.width=\"150px\"\n",
        "          btn = widgets.Button(description=\"Delete\")\n",
        "          btn.tag = os.path.join(path, filename)\n",
        "          btn.on_click(click_handler)\n",
        "          box = widgets.VBox(children=(img,btn))\n",
        "          box.layout.margin = \"5px\"\n",
        "          items.append(box)\n",
        "  \n",
        "  grid = widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(4, 25%)\"))\n",
        "  grid.layout.margin = \"15px\"\n",
        "  display(grid)\n",
        "\n",
        "files = [o for o in glob.glob(BASE_FOLDER + \"/*\") if os.path.isfile(o)]\n",
        "folders = next(os.walk(BASE_FOLDER))[1]\n",
        "folders = [f for f in folders if f[0] != \".\"]\n",
        "folders.sort()\n",
        "if(len(files) > 0): folders = [\"/\"] + folders\n",
        "\n",
        "w = interactive(render_image_cleaner, folder=folders)\n",
        "display(w)\n",
        "display(w.children[0]) # dropdown top & bottom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6KTdjEY0Fix",
        "colab_type": "text"
      },
      "source": [
        "**Run this cell to create a zip file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8CT2yVzfZXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -f {ZIP_NAME}\n",
        "!zip -q -r {ZIP_NAME} images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmtxy9LX2M1J",
        "colab_type": "text"
      },
      "source": [
        "**Run one of these cells to get your zip file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmnndlPN1yTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download to your local system\n",
        "download_file(ZIP_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BfkU4_u2kdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy to google drive \n",
        "transfer_to_drive(ZIP_NAME, dest_folder=\"Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvM0imifGwOY",
        "colab_type": "text"
      },
      "source": [
        "# Create a CSV file of URLs\n",
        "\n",
        "If you'd rather distribute a file with the image URLs and labels and have people download the images themselves you can do so here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_VQvB9KG0v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CSV_NAME = \"images.csv\" #change this to something more meaningful\n",
        "\n",
        "!rm -f {CSV_NAME}\n",
        "\n",
        "# save_urls(CSV_NAME, duckduckgo_scrape_urls, \"dogs\", \"dogs or puppies\", 10)\n",
        "# save_urls(CSV_NAME, duckduckgo_scrape_urls, \"cats\", \"cats or kittens\", 10)\n",
        "# save_urls(CSV_NAME, duckduckgo_scrape_urls, \"rabbits\", \"rabbits sitting in mugs\", 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3qXa6FLPfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download to your local system\n",
        "download_file(CSV_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e2GI-iegG3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy to google drive \n",
        "transfer_to_drive(CSV_NAME, dest_folder=\"Datasets\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}